function useTensor(xTrain, yTrain, xTest, yTest; varCount = 0)
    
    yTrain1 = convert(Array{Float32}, yTrain[:,:]);
    
    m,n =size(xTrain) 
    m1 = nrow(xTest)
     
    dfNames = names(xTrain);

    xTrain1 = convert(Array{Float32}, xTrain);
    xTest1= convert(Array{Float32},xTest)
  
    r,c = size(xTrain1)
  
    # ind_min to calculate the index of minimum t-value intialised with 0
    ind_min = 0;
    
    # If No varcount is given by the user,
    if (varCount == 0)
        varCount = c;
    end
    
    # Calculation of Weights and bias terms
    while(c >= varCount)
        
        # Starting the TF session.
        tfsession = Session(Graph())
        
        # Placeholder to hold the values of X and Y, typical in TensorFlow
        X = placeholder(Float32, shape=[r,c])
        Y = placeholder(Float32, shape=[r,1])
        
        # Putting the variables to sxtore the weights and bias term.
        W = Variable(rand(c, 1) * (-0.2-0.2) - 0.2) # Just a random value
        b = Variable(rand(1, 1) * (-0.2-0.2) - 0.2)
        
        # Defining the linear model
        linear_model = X*W + b
        
        # Definition of the cost function
        loss = reduce_sum(((linear_model - Y).^2)./(2*r))
        
        # Input of the "learning rate" and minimisation of loss.
	lRate = 0.0000000001  
	optimize = train.GradientDescentOptimizer(lRate)
        alg = train.minimize(optimize, loss)
        
        # Initialising Variables W and b
        run(tfsession, global_variables_initializer())

        # Running the iteration
        for i in 1:1000
            run(tfsession, alg, Dict(X=>xTrain1, Y=>yTrain1))
        end
    
        # Obtaining current  (Weight, Bias, Loss)
	global curr_W, curr_b, curr_loss, t_stat
	curr_W, curr_b, curr_loss = run(tfsession, [W, b, loss], Dict(X=>xTrain1, Y=>yTrain1))


        # Prediction
        Y_pred = xTrain1 * curr_W .+ curr_b
        
        # SSE error
        SSE = sum((yTrain1 - Y_pred).^2) 

        # Add teh first col
        X_1 = ([1; (xTrain1')])'
        
        # Degree of freedom
        dOF = r-c-1 
        
        # Error mean square
        MS = SSE/dOF
        
        # Standard error in each variables
        Std_err =(diag(inv(X_1'*X_1))) .* MS
        
        # Weights Including bias term
        Beta = [curr_b; curr_W]
        
        # t_stat including bias term
        t_statb = Beta ./ Std_err
        
        # t_stat without bias term
        t_stat = t_statb[1:end .!=1, :]
        
        # Index of minimum absolute value in t_stat
        ind_min = indmin(abs.(t_stat))
        
        # Decrement the value of c
        c = c-1
        
        # Deleting the column with index ind_min from colname, X_train1, X_test1
        if (c >= varCount)
            dfNames = deleteat!(dfNames, ind_min)
            xTrain1 = xTrain1[:, 1:end .!=ind_min]
            xTest1 = xTest1[:, 1:end .!=ind_min]
        end

    end

    
    
    daf = DataFrame(Variables = dfNames, Weights = curr_W[:,1], t_statistics = t_stat[:,1]);
    showall(daf)

    predictedVals = xTest1*curr_W .+ curr_b ;
    
    predictedResultDf = DataFrame(Array(predictedVals)) ;
    
    Y_1 = convert(Array, yTest);
    
    SSEtest = sum((Y_1-predictedVals).^2)
    
    print("\nSum of Square Error in Test Set : $(SSEtest) \n")
    print("\nbias term : $(curr_b)\n")
    
    CSV.write("predictedResults.csv", predictedResultDf);
    
end

